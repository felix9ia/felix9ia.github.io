(window.webpackJsonp=window.webpackJsonp||[]).push([[66],{539:function(a,e,t){"use strict";t.r(e);var s=t(1),r=Object(s.a)({},(function(){var a=this,e=a.$createElement,t=a._self._c||e;return t("ContentSlotsDistributor",{attrs:{"slot-key":a.$parent.slotKey}},[t("h1",{attrs:{id:"spark-sql-查询-es-大数据量的问题"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#spark-sql-查询-es-大数据量的问题"}},[a._v("#")]),a._v(" Spark-SQL 查询 ES 大数据量的问题")]),a._v(" "),t("p",[a._v("开发者在"),t("a",{attrs:{href:"https://discuss.elastic.co/t/how-to-scroll-through-an-elasticsearch-index-using-elasticsearch-spark/144618",target:"_blank",rel:"noopener noreferrer"}},[a._v("How to scroll through an Elasticsearch index using elasticsearch-spark?"),t("OutboundLink")],1),a._v(" 回复的原文是：")]),a._v(" "),t("div",{staticClass:"language- line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[a._v("ES-Hadoop uses the scroll endpoint to collect all the data for processing within Spark. ES-Hadoop performs the multiple scroll requests under the hood on its own, requesting the next scroll entry after the data in the current scroll response is fully consumed. I'm not sure I understand what you're looking for in terms of advancing the scroll request on your own. Could you elaborate on your use case?\n")])]),a._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[a._v("1")]),t("br")])]),t("p",[a._v("所以大数据量下，"),t("code",[a._v("ES-Hadoop")]),a._v(" 是会自己启动 scroll 去查询所有的数据的。所以完全不用担心大数据量的问题。")]),a._v(" "),t("h2",{attrs:{id:"异常"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#异常"}},[a._v("#")]),a._v(" 异常")]),a._v(" "),t("p",[a._v("当遇到 kafka 无法存的问题时，去 google 搜索 "),t("a",{attrs:{href:"https://www.google.com/search?q=java+maven+Failed+to+find+data+source%3A+kafka+Structured+Streaming+%2B+Kafka+Integration+Guide&newwindow=1&sxsrf=ALiCzsaf1uDlMPjK0JZ4PXhmlMazIRbKmg%3A1660114229270&ei=NVXzYtKMEMj8hwOf7JLgCg&ved=0ahUKEwiS-6-317v5AhVI_mEKHR-2BKwQ4dUDCA4&uact=5&oq=java+maven+Failed+to+find+data+source%3A+kafka+Structured+Streaming+%2B+Kafka+Integration+Guide&gs_lcp=Cgdnd3Mtd2l6EAM6BwgAEEcQsANKBQg8EgE1SgQIQRgASgQIRhgAUIQIWJgqYKAtaAVwAXgBgAHYAogBrRaSAQgwLjEwLjQuMZgBAKABAcgBCMABAQ&sclient=gws-wiz",target:"_blank",rel:"noopener noreferrer"}},[a._v("java maven Failed to find data source: kafka Structured Streaming + Kafka Integration Guide"),t("OutboundLink")],1),a._v(" 得到了"),t("a",{attrs:{href:"https://intellipaat.com/community/16747/why-does-spark-application-fail-with-classnotfoundexception-failed-to-find-data-source-kafka-as-uber-jar-with-sbt-assembly",target:"_blank",rel:"noopener noreferrer"}},[a._v("[Why does Spark application fail with “ClassNotFoundException: Failed to find data source: kafka” as uber-jar with sbt assembly?]https://intellipaat.com/community/16747/why-does-spark-application-fail-with-classnotfoundexception-failed-to-find-data-source-kafka-as-uber-jar-with-sbt-assembly"),t("OutboundLink")],1)]),a._v(" "),t("p",[a._v("最终通过指定 "),t("code",[a._v('.format("org.apache.spark.sql.kafka010.KafkaSourceProvider")')]),a._v(" 解决了")]),a._v(" "),t("div",{staticClass:"language- line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[a._v('            mergedAll\n                    // .toDF()\n                    .selectExpr("CAST(uid AS STRING) AS key", "to_json(struct(*)) AS value")\n                    // .selectExpr("CAST(key AS STRING)", "CAST(value) AS STRING")\n                    .write()\n                    // .format("kafka")\n                    .format("org.apache.spark.sql.kafka010.KafkaSourceProvider")\n                    .option("kafka.bootstrap.servers", kafkaAddresses)\n                    .option("topic", topicMetric)\n                    .save();\n')])]),a._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[a._v("1")]),t("br"),t("span",{staticClass:"line-number"},[a._v("2")]),t("br"),t("span",{staticClass:"line-number"},[a._v("3")]),t("br"),t("span",{staticClass:"line-number"},[a._v("4")]),t("br"),t("span",{staticClass:"line-number"},[a._v("5")]),t("br"),t("span",{staticClass:"line-number"},[a._v("6")]),t("br"),t("span",{staticClass:"line-number"},[a._v("7")]),t("br"),t("span",{staticClass:"line-number"},[a._v("8")]),t("br"),t("span",{staticClass:"line-number"},[a._v("9")]),t("br"),t("span",{staticClass:"line-number"},[a._v("10")]),t("br")])]),t("h2",{attrs:{id:"参考"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#参考"}},[a._v("#")]),a._v(" 参考")]),a._v(" "),t("p",[t("a",{attrs:{href:"https://cloud.tencent.com/developer/article/1780508",target:"_blank",rel:"noopener noreferrer"}},[a._v("看了这篇博客，你还敢说不会Structured Streaming?"),t("OutboundLink")],1)]),a._v(" "),t("p",[a._v("["),t("a",{attrs:{href:"https://stackoverflow.com/questions/58877005/issue-in-spark-kafka-integration",target:"_blank",rel:"noopener noreferrer"}},[a._v("Issue in Spark + kafka Integration"),t("OutboundLink")],1),a._v("](https://stackoverflow.com/questions/58877005/issue-in-spark-kafka-integration)")])])}),[],!1,null,null,null);e.default=r.exports}}]);